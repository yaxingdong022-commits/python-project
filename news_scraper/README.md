# News Scraper - Comprehensive Web Scraping Project

A production-ready web scraping project built with Scrapy framework for extracting news articles from major news websites (BBC, CNN), with data analysis and visualization capabilities.

## ğŸŒŸ Features

- **Web Scraping**: Scrapy-based spider for BBC News (easily extensible to other sources)
- **Smart Filtering**: Extracts articles from the last 7 days
- **Data Storage**: SQLite database for persistent storage
- **Pagination Handling**: Automatically follows pagination links
- **User-Agent Rotation**: Prevents IP bans using rotating user agents
- **Error Logging**: Comprehensive logging system
- **Data Analysis**: Pandas-based analysis with keyword extraction
- **NLP Integration**: NLTK for natural language processing and keyword extraction
- **Visualization**: Matplotlib charts for keyword frequency and trends
- **CLI Tool**: User-friendly command-line interface with argparse
- **Testing**: Comprehensive test suite with pytest
- **Modern Python**: Uses Python 3.12 features including async support
- **Modular Design**: Clean, maintainable code structure

## ğŸ“‹ Requirements

- Python 3.12+
- See `requirements.txt` for dependencies

## ğŸš€ Installation

1. **Clone the repository**:
```bash
git clone <repository-url>
cd news_scraper
```

2. **Create a virtual environment** (recommended):
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Download NLTK data** (if not auto-downloaded):
```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

## ğŸ“– Usage

The news scraper provides a command-line interface with several commands:

### Quick Start - Run Complete Pipeline

```bash
python -m news_scraper.cli run-all
```

This will:
1. Scrape articles from BBC News
2. Analyze the data and extract keywords
3. Generate visualizations
4. Create a CSV report

### Individual Commands

#### 1. Scrape News Articles

```bash
# Scrape BBC news (default: last 7 days)
python -m news_scraper.cli scrape --spider bbc

# Scrape with custom time range
python -m news_scraper.cli scrape --spider bbc --days 14
```

#### 2. Analyze Scraped Data

```bash
# Analyze articles and generate report
python -m news_scraper.cli analyze

# Customize number of keywords
python -m news_scraper.cli analyze --top-keywords 30
```

#### 3. Create Visualizations

```bash
# Generate keyword frequency charts
python -m news_scraper.cli visualize

# Specify custom output directory
python -m news_scraper.cli visualize --output-dir ./output
```

### Help

```bash
python -m news_scraper.cli --help
python -m news_scraper.cli scrape --help
```

## ğŸ“‚ Project Structure

```
news_scraper/
â”œâ”€â”€ news_scraper/           # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py        # Module entry point
â”‚   â”œâ”€â”€ cli.py             # Command-line interface
â”‚   â”œâ”€â”€ items.py           # Scrapy item definitions
â”‚   â”œâ”€â”€ settings.py        # Scrapy settings
â”‚   â”œâ”€â”€ pipelines.py       # Data processing pipelines
â”‚   â”œâ”€â”€ middlewares.py     # Custom middlewares
â”‚   â”œâ”€â”€ analyzer.py        # Data analysis module
â”‚   â”œâ”€â”€ visualizer.py      # Visualization module
â”‚   â””â”€â”€ spiders/           # Spider implementations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ bbc_spider.py  # BBC News spider
â”œâ”€â”€ tests/                 # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_analyzer.py
â”‚   â”œâ”€â”€ test_items.py
â”‚   â”œâ”€â”€ test_pipelines.py
â”‚   â””â”€â”€ test_visualizer.py
â”œâ”€â”€ data/                  # Generated data (created at runtime)
â”‚   â”œâ”€â”€ news_articles.db   # SQLite database
â”‚   â”œâ”€â”€ analysis_report.csv
â”‚   â””â”€â”€ keyword_frequency.png
â”œâ”€â”€ logs/                  # Log files
â”‚   â””â”€â”€ scraper.log
â”œâ”€â”€ scrapy.cfg             # Scrapy configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ pytest.ini            # Pytest configuration
â”œâ”€â”€ .gitignore           # Git ignore rules
â””â”€â”€ README.md            # This file
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=news_scraper

# Run specific test file
pytest tests/test_analyzer.py

# Run with verbose output
pytest -v
```

## ğŸ”§ Configuration

### Scrapy Settings (`news_scraper/settings.py`)

Key configurable parameters:

- `DAYS_TO_SCRAPE`: Number of days to scrape (default: 7)
- `CONCURRENT_REQUESTS`: Number of concurrent requests (default: 16)
- `DOWNLOAD_DELAY`: Delay between requests in seconds (default: 2)
- `DATABASE_PATH`: Path to SQLite database (default: data/news_articles.db)
- `LOG_LEVEL`: Logging level (default: INFO)

### Database Schema

The SQLite database contains a single `articles` table:

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Primary key |
| title | TEXT | Article title |
| summary | TEXT | Article summary |
| publication_date | TEXT | Publication date |
| author | TEXT | Article author |
| url | TEXT | Article URL (unique) |
| scraped_at | TEXT | Timestamp when scraped |

## ğŸ“Š Output Files

After running the scraper and analysis:

1. **Database**: `data/news_articles.db` - SQLite database with all articles
2. **CSV Report**: `data/analysis_report.csv` - Detailed analysis report
3. **Visualizations**:
   - `data/keyword_frequency.png` - Bar chart of top keywords
   - `data/articles_per_author.png` - Author statistics
   - `data/articles_timeline.png` - Timeline of articles

## ğŸ›¡ï¸ Anti-Ban Mechanisms

The scraper implements several strategies to avoid being banned:

1. **User-Agent Rotation**: Random user agents for each request
2. **Request Delays**: Configurable delays between requests
3. **AutoThrottle**: Automatic throttling based on server load
4. **Robots.txt Compliance**: Respects robots.txt rules
5. **Concurrent Request Limits**: Controlled request rate

## ğŸ” Data Analysis Features

### Keyword Extraction

Uses NLTK for advanced text processing:
- Tokenization
- Stop word removal (English + custom news stopwords)
- Frequency analysis
- Top N keywords extraction

### Visualizations

- **Keyword Frequency**: Horizontal bar chart showing most common keywords
- **Author Statistics**: Articles per author
- **Timeline**: Publication trends over time

## ğŸ› Error Handling

The scraper includes comprehensive error handling:

- **Logging**: All errors logged to `logs/scraper.log`
- **Exception Handling**: Graceful handling of network errors
- **Data Validation**: Validates scraped data before storage
- **Retry Mechanism**: Automatic retries for failed requests

## ğŸš¦ Logging

Logs are written to:
- Console: INFO level and above
- File: `logs/scraper.log` - All levels

Log format:
```
YYYY-MM-DD HH:MM:SS [logger_name] LEVEL: message
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `pytest`
5. Submit a pull request

## ğŸ“ Adding New Spiders

To add a spider for a different news source:

1. Create a new spider file in `news_scraper/spiders/`
2. Inherit from `scrapy.Spider`
3. Implement `parse()` and `parse_article()` methods
4. Update CLI to include the new spider

Example:
```python
# news_scraper/spiders/cnn_spider.py
class CNNSpider(scrapy.Spider):
    name = "cnn"
    allowed_domains = ["cnn.com"]
    start_urls = ["https://www.cnn.com/"]
    
    def parse(self, response):
        # Extract article links
        pass
    
    def parse_article(self, response):
        # Extract article details
        pass
```

## ğŸ” Best Practices

1. **Rate Limiting**: Always respect the website's rate limits
2. **Robots.txt**: Check and follow robots.txt rules
3. **Data Privacy**: Handle scraped data responsibly
4. **Legal Compliance**: Ensure scraping complies with terms of service
5. **Resource Management**: Close database connections properly

## ğŸ Troubleshooting

### Common Issues

**Issue**: `ModuleNotFoundError: No module named 'scrapy'`
- **Solution**: Install dependencies: `pip install -r requirements.txt`

**Issue**: NLTK data not found
- **Solution**: Download NLTK data: `python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"`

**Issue**: Permission error writing to database
- **Solution**: Ensure write permissions for `data/` directory

**Issue**: No articles scraped
- **Solution**: Check internet connection and website availability

## ğŸ“ˆ Performance

- **Scraping Speed**: ~50-100 articles per minute (depends on settings)
- **Memory Usage**: ~50-100 MB typical
- **Storage**: ~1 KB per article in database

## ğŸ¯ Future Enhancements

- [ ] Support for more news sources (CNN, Reuters, etc.)
- [ ] Real-time scraping with scheduling
- [ ] Advanced NLP analysis (sentiment analysis, entity extraction)
- [ ] Web dashboard for viewing results
- [ ] Export to additional formats (JSON, Excel)
- [ ] Email notifications
- [ ] Distributed scraping with Scrapy Cloud

## ğŸ“„ License

This project is provided as-is for educational purposes.

## ğŸ‘¥ Authors

Created as a comprehensive demonstration of Python web scraping capabilities.

## ğŸ™ Acknowledgments

- **Scrapy**: Powerful web scraping framework
- **Pandas**: Data analysis library
- **NLTK**: Natural language toolkit
- **Matplotlib**: Visualization library

## ğŸ“ Support

For issues, questions, or contributions, please open an issue on GitHub.

---

**Happy Scraping! ğŸš€**
