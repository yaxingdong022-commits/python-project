"""
ç¬¬äºŒå…³ï¼šHTMLè§£æ
å­¦ä¹ ç›®æ ‡ï¼š
1. ä½¿ç”¨ lxml å’Œ XPath è§£æ HTML
2. ä½¿ç”¨ BeautifulSoup è§£æ HTML
3. æå–ç‰¹å®šçš„æ•°æ®ï¼ˆæ ‡é¢˜ã€é“¾æ¥ã€å›¾ç‰‡ç­‰ï¼‰
4. æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
"""

import requests
from lxml import etree
from bs4 import BeautifulSoup
from utils.request_helper import make_request
from utils.parser_helper import extract_text, extract_links, clean_text
from utils.storage_helper import save_to_json


def parse_with_xpath_example():
    """ä½¿ç”¨XPathè§£æHTMLç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹1: ä½¿ç”¨XPathè§£æHTML")
    print("=" * 60)
    
    url = 'http://quotes.toscrape.com/'
    
    response = make_request(url)
    if not response:
        print("è¯·æ±‚å¤±è´¥")
        return
    
    # ä½¿ç”¨lxmlè§£æHTML
    tree = etree.HTML(response.text)
    
    # ä½¿ç”¨XPathæå–åè¨€
    quotes = tree.xpath('//span[@class="text"]/text()')
    authors = tree.xpath('//small[@class="author"]/text()')
    
    print(f"ä» {url} æå–åˆ° {len(quotes)} æ¡åè¨€:\n")
    
    for i, (quote, author) in enumerate(zip(quotes[:5], authors[:5]), 1):
        print(f"{i}. {quote}")
        print(f"   â€” {author}\n")
    
    print()


def parse_with_beautifulsoup_example():
    """ä½¿ç”¨BeautifulSoupè§£æHTMLç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹2: ä½¿ç”¨BeautifulSoupè§£æHTML")
    print("=" * 60)
    
    url = 'http://quotes.toscrape.com/'
    
    response = make_request(url)
    if not response:
        print("è¯·æ±‚å¤±è´¥")
        return
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(response.text, 'lxml')
    
    # æŸ¥æ‰¾æ‰€æœ‰åè¨€div
    quote_divs = soup.find_all('div', class_='quote')
    
    print(f"ä» {url} ä½¿ç”¨BeautifulSoupæå–æ•°æ®:\n")
    
    quotes_data = []
    for i, div in enumerate(quote_divs[:5], 1):
        quote = div.find('span', class_='text').get_text()
        author = div.find('small', class_='author').get_text()
        tags = [tag.get_text() for tag in div.find_all('a', class_='tag')]
        
        quotes_data.append({
            'quote': quote,
            'author': author,
            'tags': tags
        })
        
        print(f"{i}. {quote}")
        print(f"   ä½œè€…: {author}")
        print(f"   æ ‡ç­¾: {', '.join(tags)}\n")
    
    # ä¿å­˜æ•°æ®
    save_to_json(quotes_data, 'level2_quotes.json')
    print()


def extract_links_example():
    """æå–é“¾æ¥ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹3: æå–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥")
    print("=" * 60)
    
    url = 'http://quotes.toscrape.com/'
    
    response = make_request(url)
    if not response:
        print("è¯·æ±‚å¤±è´¥")
        return
    
    # ä½¿ç”¨BeautifulSoupæå–é“¾æ¥
    soup = BeautifulSoup(response.text, 'lxml')
    
    # æå–æ‰€æœ‰ä½œè€…é“¾æ¥
    author_links = []
    for link in soup.find_all('a'):
        href = link.get('href', '')
        if '/author/' in href:
            full_url = f"http://quotes.toscrape.com{href}"
            author_name = link.get_text()
            author_links.append({
                'name': author_name,
                'url': full_url
            })
    
    # å»é‡
    unique_authors = {item['url']: item for item in author_links}.values()
    
    print(f"æ‰¾åˆ° {len(unique_authors)} ä¸ªä½œè€…é“¾æ¥:\n")
    
    for i, author in enumerate(list(unique_authors)[:5], 1):
        print(f"{i}. {author['name']}")
        print(f"   é“¾æ¥: {author['url']}\n")
    
    print()


def scrape_book_data():
    """çˆ¬å–å›¾ä¹¦æ•°æ®ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹4: çˆ¬å–å›¾ä¹¦ç½‘ç«™æ•°æ®")
    print("=" * 60)
    
    url = 'http://books.toscrape.com/'
    
    response = make_request(url)
    if not response:
        print("è¯·æ±‚å¤±è´¥")
        return
    
    soup = BeautifulSoup(response.text, 'lxml')
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾ä¹¦
    books = soup.find_all('article', class_='product_pod')
    
    print(f"ä» {url} æå–å›¾ä¹¦ä¿¡æ¯:\n")
    
    books_data = []
    for i, book in enumerate(books[:5], 1):
        # æå–æ ‡é¢˜
        title = book.find('h3').find('a').get('title')
        
        # æå–ä»·æ ¼
        price = book.find('p', class_='price_color').get_text()
        
        # æå–è¯„åˆ†
        star_class = book.find('p', class_='star-rating').get('class')
        rating = star_class[1] if len(star_class) > 1 else 'N/A'
        
        books_data.append({
            'title': title,
            'price': price,
            'rating': rating
        })
        
        print(f"{i}. {title}")
        print(f"   ä»·æ ¼: {price}")
        print(f"   è¯„åˆ†: {rating}\n")
    
    # ä¿å­˜æ•°æ®
    save_to_json(books_data, 'level2_books.json')
    print()


def data_cleaning_example():
    """æ•°æ®æ¸…æ´—ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹5: æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè„æ•°æ®
    dirty_data = [
        "  è¿™æ˜¯ä¸€ä¸ªæœ‰å¤šä½™ç©ºæ ¼çš„æ–‡æœ¬  \n\n",
        "\t\tå¸¦æœ‰åˆ¶è¡¨ç¬¦çš„æ–‡æœ¬\t\t",
        "å¤šä¸ª    ç©ºæ ¼    çš„    æ–‡æœ¬",
        "æ­£å¸¸çš„æ–‡æœ¬"
    ]
    
    print("åŸå§‹æ•°æ®:")
    for i, data in enumerate(dirty_data, 1):
        print(f"{i}. '{data}'")
    
    print("\næ¸…æ´—åçš„æ•°æ®:")
    cleaned_data = [clean_text(data) for data in dirty_data]
    for i, data in enumerate(cleaned_data, 1):
        print(f"{i}. '{data}'")
    
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("ğŸ“ æ¬¢è¿æ¥åˆ°ç¬¬äºŒå…³ï¼šHTMLè§£æ")
    print("ğŸ“š åœ¨è¿™ä¸€å…³ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•è§£æHTMLå¹¶æå–æ•°æ®")
    print("\n")
    
    try:
        parse_with_xpath_example()
        parse_with_beautifulsoup_example()
        extract_links_example()
        scrape_book_data()
        data_cleaning_example()
        
        print("=" * 60)
        print("ğŸ‰ æ­å–œï¼ä½ å·²å®Œæˆç¬¬äºŒå…³çš„å­¦ä¹ ")
        print("ğŸ’¡ ç°åœ¨ä½ å·²ç»æŒæ¡äº†ï¼š")
        print("   âœ“ ä½¿ç”¨XPathè§£æHTML")
        print("   âœ“ ä½¿ç”¨BeautifulSoupè§£æHTML")
        print("   âœ“ æå–é“¾æ¥å’Œæ•°æ®")
        print("   âœ“ æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–")
        print("   âœ“ ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶")
        print()
        print("ğŸš€ å‡†å¤‡å¥½è¿›å…¥ç¬¬ä¸‰å…³äº†å—ï¼Ÿè¿è¡Œ level3_advanced_crawler.py")
        print("=" * 60)
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
