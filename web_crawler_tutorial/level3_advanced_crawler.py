"""
ç¬¬ä¸‰å…³ï¼šé«˜çº§çˆ¬è™«æŠ€æœ¯
å­¦ä¹ ç›®æ ‡ï¼š
1. å®ç°å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
2. æ·»åŠ æ—¥å¿—è®°å½•
3. å®ç°å¤šé¡µé¢çˆ¬å–
4. åº”å¯¹åçˆ¬è™«ç­–ç•¥
5. æ•°æ®æŒä¹…åŒ–å­˜å‚¨
"""

import time
import logging
import random
from typing import List, Dict
from bs4 import BeautifulSoup
from utils.request_helper import make_request, get_random_user_agent
from utils.storage_helper import save_to_json, save_to_csv, ensure_dir
from config.settings import REQUEST_DELAY


# é…ç½®æ—¥å¿—
def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    ensure_dir('logs')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/crawler.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


logger = setup_logger()


def crawl_with_retry(url: str, max_retries: int = 3) -> str:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–
    
    Args:
        url: ç›®æ ‡URL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        HTMLå†…å®¹
    """
    logger.info(f"å¼€å§‹çˆ¬å–: {url}")
    
    for attempt in range(max_retries):
        try:
            response = make_request(url)
            if response:
                logger.info(f"âœ“ çˆ¬å–æˆåŠŸ: {url}")
                return response.text
            else:
                logger.warning(f"âœ— çˆ¬å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {url}")
                
        except Exception as e:
            logger.error(f"âœ— å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
        
        if attempt < max_retries - 1:
            wait_time = (attempt + 1) * 2
            logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    logger.error(f"âœ— è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ: {url}")
    return None


def crawl_multiple_pages(base_url: str, max_pages: int = 5) -> List[Dict]:
    """
    çˆ¬å–å¤šä¸ªé¡µé¢
    
    Args:
        base_url: åŸºç¡€URL
        max_pages: æœ€å¤§é¡µæ•°
    
    Returns:
        æ‰€æœ‰é¡µé¢çš„æ•°æ®
    """
    logger.info(f"å¼€å§‹çˆ¬å–å¤šä¸ªé¡µé¢ï¼Œæœ€å¤š {max_pages} é¡µ")
    
    all_quotes = []
    
    for page in range(1, max_pages + 1):
        url = f"{base_url}/page/{page}/"
        logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")
        
        html = crawl_with_retry(url)
        if not html:
            logger.warning(f"é¡µé¢ {page} çˆ¬å–å¤±è´¥ï¼Œè·³è¿‡")
            continue
        
        # è§£ææ•°æ®
        soup = BeautifulSoup(html, 'lxml')
        quote_divs = soup.find_all('div', class_='quote')
        
        if not quote_divs:
            logger.info(f"é¡µé¢ {page} æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
            break
        
        logger.info(f"ä»ç¬¬ {page} é¡µæå–åˆ° {len(quote_divs)} æ¡æ•°æ®")
        
        for div in quote_divs:
            try:
                quote = div.find('span', class_='text').get_text()
                author = div.find('small', class_='author').get_text()
                tags = [tag.get_text() for tag in div.find_all('a', class_='tag')]
                
                all_quotes.append({
                    'page': page,
                    'quote': quote,
                    'author': author,
                    'tags': tags
                })
            except Exception as e:
                logger.error(f"è§£ææ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        if page < max_pages:
            delay = REQUEST_DELAY + random.uniform(0, 1)
            logger.info(f"ç­‰å¾… {delay:.2f} ç§’...")
            time.sleep(delay)
    
    logger.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_quotes)} æ¡æ•°æ®")
    return all_quotes


def anti_spider_strategy():
    """
    åçˆ¬è™«ç­–ç•¥ç¤ºä¾‹
    """
    print("=" * 60)
    print("ç¤ºä¾‹1: åçˆ¬è™«ç­–ç•¥")
    print("=" * 60)
    
    logger.info("æ¼”ç¤ºåçˆ¬è™«ç­–ç•¥")
    
    strategies = [
        "1. ä½¿ç”¨éšæœºUser-Agent",
        "2. æ·»åŠ è¯·æ±‚å»¶è¿Ÿ",
        "3. ä½¿ç”¨ä»£ç†IPï¼ˆå¦‚æœéœ€è¦ï¼‰",
        "4. æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼ˆéšæœºå»¶è¿Ÿï¼‰",
        "5. éµå®ˆrobots.txt",
    ]
    
    print("\nå¸¸ç”¨åçˆ¬è™«ç­–ç•¥:")
    for strategy in strategies:
        print(f"  {strategy}")
    
    # æ¼”ç¤ºéšæœºUser-Agent
    print("\néšæœºUser-Agentç¤ºä¾‹:")
    for i in range(3):
        ua = get_random_user_agent()
        print(f"  {i+1}. {ua[:50]}...")
    
    print()


def crawl_quotes_example():
    """çˆ¬å–åè¨€ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹2: çˆ¬å–å¤šé¡µåè¨€æ•°æ®")
    print("=" * 60)
    
    base_url = 'http://quotes.toscrape.com'
    
    # çˆ¬å–å‰3é¡µ
    quotes = crawl_multiple_pages(base_url, max_pages=3)
    
    if quotes:
        # ä¿å­˜ä¸ºJSON
        save_to_json(quotes, 'level3_quotes_full.json')
        
        # ä¿å­˜ä¸ºCSV
        save_to_csv(quotes, 'level3_quotes_full.csv')
        
        # ç»Ÿè®¡ä¿¡æ¯
        authors = set(q['author'] for q in quotes)
        all_tags = set()
        for q in quotes:
            all_tags.update(q['tags'])
        
        print("\nç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»å…±çˆ¬å–: {len(quotes)} æ¡åè¨€")
        print(f"  æ¶‰åŠä½œè€…: {len(authors)} ä½")
        print(f"  æ‰€æœ‰æ ‡ç­¾: {len(all_tags)} ä¸ª")
        print(f"\nçƒ­é—¨ä½œè€…:")
        
        # ç»Ÿè®¡ä½œè€…å‡ºç°æ¬¡æ•°
        author_counts = {}
        for q in quotes:
            author = q['author']
            author_counts[author] = author_counts.get(author, 0) + 1
        
        # æ’åºå¹¶æ˜¾ç¤ºå‰5å
        top_authors = sorted(author_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for i, (author, count) in enumerate(top_authors, 1):
            print(f"  {i}. {author}: {count} æ¡")
    
    print()


def error_handling_example():
    """é”™è¯¯å¤„ç†ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹3: å®Œå–„çš„é”™è¯¯å¤„ç†")
    print("=" * 60)
    
    # æµ‹è¯•å„ç§é”™è¯¯åœºæ™¯
    test_urls = [
        ('http://quotes.toscrape.com/', 'æ­£å¸¸URL'),
        ('http://quotes.toscrape.com/page/999/', 'ä¸å­˜åœ¨çš„é¡µé¢'),
        ('http://this-domain-does-not-exist-12345.com/', 'ä¸å­˜åœ¨çš„åŸŸå'),
    ]
    
    for url, description in test_urls:
        print(f"\næµ‹è¯•: {description}")
        print(f"URL: {url}")
        
        try:
            html = crawl_with_retry(url, max_retries=2)
            if html:
                print(f"âœ“ æˆåŠŸè·å–å†…å®¹ ({len(html)} å­—ç¬¦)")
            else:
                print("âœ— è·å–å¤±è´¥")
        except Exception as e:
            print(f"âœ— å¼‚å¸¸: {e}")
    
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("ğŸ“ æ¬¢è¿æ¥åˆ°ç¬¬ä¸‰å…³ï¼šé«˜çº§çˆ¬è™«æŠ€æœ¯")
    print("ğŸ“š åœ¨è¿™ä¸€å…³ï¼Œä½ å°†å­¦ä¹ æ›´é«˜çº§çš„çˆ¬è™«æŠ€å·§")
    print("\n")
    
    try:
        anti_spider_strategy()
        crawl_quotes_example()
        error_handling_example()
        
        print("=" * 60)
        print("ğŸ‰ æ­å–œï¼ä½ å·²å®Œæˆç¬¬ä¸‰å…³çš„å­¦ä¹ ")
        print("ğŸ’¡ ç°åœ¨ä½ å·²ç»æŒæ¡äº†ï¼š")
        print("   âœ“ å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        print("   âœ“ æ—¥å¿—è®°å½•")
        print("   âœ“ å¤šé¡µé¢çˆ¬å–")
        print("   âœ“ åçˆ¬è™«ç­–ç•¥")
        print("   âœ“ æ•°æ®æŒä¹…åŒ–ï¼ˆJSONå’ŒCSVï¼‰")
        print()
        print("ğŸš€ å‡†å¤‡å¥½è¿›å…¥ç¬¬å››å…³äº†å—ï¼Ÿè¿è¡Œ level4_professional_crawler.py")
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
