"""
ç¬¬å››å…³ï¼šä¸“ä¸šçº§çˆ¬è™«æ¡†æ¶
å­¦ä¹ ç›®æ ‡ï¼š
1. ä½¿ç”¨å¼‚æ­¥å¹¶å‘çˆ¬å–ï¼ˆasyncio + aiohttpï¼‰
2. å®ç°é€Ÿç‡é™åˆ¶å’Œè¯·æ±‚é˜Ÿåˆ—
3. User-Agentè½®æ¢
4. è®¾è®¡å¯æ‰©å±•çš„çˆ¬è™«æ¶æ„
5. æ€§èƒ½ä¼˜åŒ–
"""

import asyncio
import aiohttp
import time
import logging
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from utils.storage_helper import save_to_json, ensure_dir
from config.settings import USER_AGENTS, CONCURRENT_REQUESTS


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, rate: int = 10, per: float = 1.0):
        """
        Args:
            rate: é€Ÿç‡ï¼ˆæ¬¡æ•°ï¼‰
            per: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        """
        self.rate = rate
        self.per = per
        self.allowance = rate
        self.last_check = time.time()
    
    async def acquire(self):
        """è·å–ä»¤ç‰Œ"""
        current = time.time()
        time_passed = current - self.last_check
        self.last_check = current
        self.allowance += time_passed * (self.rate / self.per)
        
        if self.allowance > self.rate:
            self.allowance = self.rate
        
        if self.allowance < 1.0:
            sleep_time = (1.0 - self.allowance) * (self.per / self.rate)
            await asyncio.sleep(sleep_time)
            self.allowance = 0.0
        else:
            self.allowance -= 1.0


class AsyncCrawler:
    """å¼‚æ­¥çˆ¬è™«ç±»"""
    
    def __init__(self, max_concurrent: int = CONCURRENT_REQUESTS):
        self.max_concurrent = max_concurrent
        self.rate_limiter = RateLimiter(rate=10, per=1.0)
        self.session: Optional[aiohttp.ClientSession] = None
        self.user_agent_index = 0
    
    def get_next_user_agent(self) -> str:
        """è½®æ¢User-Agent"""
        ua = USER_AGENTS[self.user_agent_index % len(USER_AGENTS)]
        self.user_agent_index += 1
        return ua
    
    async def fetch(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥è·å–å•ä¸ªURL
        
        Args:
            url: ç›®æ ‡URL
        
        Returns:
            HTMLå†…å®¹
        """
        await self.rate_limiter.acquire()
        
        headers = {
            'User-Agent': self.get_next_user_agent()
        }
        
        try:
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    logger.info(f"âœ“ æˆåŠŸ: {url}")
                    return await response.text()
                else:
                    logger.warning(f"âœ— çŠ¶æ€ç  {response.status}: {url}")
                    return None
        except asyncio.TimeoutError:
            logger.error(f"âœ— è¶…æ—¶: {url}")
            return None
        except Exception as e:
            logger.error(f"âœ— å¼‚å¸¸ {url}: {e}")
            return None
    
    async def fetch_all(self, urls: List[str]) -> List[Optional[str]]:
        """
        å¹¶å‘è·å–å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
        
        Returns:
            HTMLå†…å®¹åˆ—è¡¨
        """
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def fetch_with_semaphore(url):
            async with semaphore:
                return await self.fetch(url)
        
        # å¹¶å‘æ‰§è¡Œ
        tasks = [fetch_with_semaphore(url) for url in urls]
        return await asyncio.gather(*tasks)
    
    async def crawl_quotes(self, max_pages: int = 5) -> List[Dict]:
        """
        å¼‚æ­¥çˆ¬å–åè¨€ç½‘ç«™
        
        Args:
            max_pages: æœ€å¤§é¡µæ•°
        
        Returns:
            æ‰€æœ‰æ•°æ®
        """
        base_url = 'http://quotes.toscrape.com'
        urls = [f"{base_url}/page/{i}/" for i in range(1, max_pages + 1)]
        
        logger.info(f"å¼€å§‹å¼‚æ­¥çˆ¬å– {len(urls)} ä¸ªé¡µé¢")
        start_time = time.time()
        
        # åˆ›å»ºä¼šè¯
        async with aiohttp.ClientSession() as session:
            self.session = session
            
            # å¹¶å‘è·å–æ‰€æœ‰é¡µé¢
            html_pages = await self.fetch_all(urls)
        
        # è§£ææ•°æ®
        all_quotes = []
        for page_num, html in enumerate(html_pages, 1):
            if html:
                quotes = self.parse_quotes_page(html, page_num)
                all_quotes.extend(quotes)
        
        elapsed = time.time() - start_time
        logger.info(f"çˆ¬å–å®Œæˆï¼å…± {len(all_quotes)} æ¡æ•°æ®ï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        
        return all_quotes
    
    def parse_quotes_page(self, html: str, page_num: int) -> List[Dict]:
        """
        è§£æå•ä¸ªé¡µé¢
        
        Args:
            html: HTMLå†…å®¹
            page_num: é¡µç 
        
        Returns:
            è¯¥é¡µçš„æ•°æ®
        """
        soup = BeautifulSoup(html, 'lxml')
        quote_divs = soup.find_all('div', class_='quote')
        
        quotes = []
        for div in quote_divs:
            try:
                quote = div.find('span', class_='text').get_text()
                author = div.find('small', class_='author').get_text()
                tags = [tag.get_text() for tag in div.find_all('a', class_='tag')]
                
                quotes.append({
                    'page': page_num,
                    'quote': quote,
                    'author': author,
                    'tags': tags
                })
            except Exception as e:
                logger.error(f"è§£æå‡ºé”™: {e}")
                continue
        
        return quotes


async def async_crawl_example():
    """å¼‚æ­¥çˆ¬å–ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹1: å¼‚æ­¥å¹¶å‘çˆ¬å–")
    print("=" * 60)
    
    crawler = AsyncCrawler(max_concurrent=5)
    
    # çˆ¬å–10é¡µ
    quotes = await crawler.crawl_quotes(max_pages=10)
    
    if quotes:
        # ä¿å­˜æ•°æ®
        save_to_json(quotes, 'level4_async_quotes.json')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nâœ“ æˆåŠŸçˆ¬å– {len(quotes)} æ¡åè¨€")
        
        # ç»Ÿè®¡ä½œè€…
        authors = {}
        for q in quotes:
            author = q['author']
            authors[author] = authors.get(author, 0) + 1
        
        print(f"âœ“ æ¶‰åŠ {len(authors)} ä½ä½œè€…")
        
        top_authors = sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]
        print("\nçƒ­é—¨ä½œè€…:")
        for i, (author, count) in enumerate(top_authors, 1):
            print(f"  {i}. {author}: {count} æ¡")
    
    print()


def rate_limiter_example():
    """é€Ÿç‡é™åˆ¶å™¨ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹2: é€Ÿç‡é™åˆ¶å™¨")
    print("=" * 60)
    
    print("\né€Ÿç‡é™åˆ¶å™¨çš„ä½œç”¨ï¼š")
    print("  â€¢ æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è¢«å°")
    print("  â€¢ ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•")
    print("  â€¢ å¯é…ç½®é€Ÿç‡ï¼ˆå¦‚ï¼šæ¯ç§’10ä¸ªè¯·æ±‚ï¼‰")
    print()


def architecture_example():
    """æ¶æ„è®¾è®¡ç¤ºä¾‹"""
    print("=" * 60)
    print("ç¤ºä¾‹3: ä¸“ä¸šçˆ¬è™«æ¶æ„è®¾è®¡")
    print("=" * 60)
    
    print("\nä¸“ä¸šçˆ¬è™«æ¶æ„ç»„æˆï¼š")
    components = [
        "1. è°ƒåº¦å™¨ (Scheduler) - ç®¡ç†URLé˜Ÿåˆ—",
        "2. ä¸‹è½½å™¨ (Downloader) - å¼‚æ­¥ä¸‹è½½ç½‘é¡µ",
        "3. è§£æå™¨ (Parser) - è§£æHTMLå¹¶æå–æ•°æ®",
        "4. ç®¡é“ (Pipeline) - æ•°æ®æ¸…æ´—å’Œå­˜å‚¨",
        "5. ä¸­é—´ä»¶ (Middleware) - å¤„ç†è¯·æ±‚/å“åº”",
        "6. å»é‡å™¨ (Deduplicator) - é¿å…é‡å¤çˆ¬å–",
    ]
    
    for component in components:
        print(f"  {component}")
    
    print("\næ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼š")
    tips = [
        "â€¢ ä½¿ç”¨å¼‚æ­¥IOï¼ˆasyncio + aiohttpï¼‰",
        "â€¢ è®¾ç½®åˆç†çš„å¹¶å‘æ•°",
        "â€¢ ä½¿ç”¨è¿æ¥æ± ",
        "â€¢ å®ç°æ–­ç‚¹ç»­ä¼ ",
        "â€¢ ä½¿ç”¨ç¼“å­˜æœºåˆ¶",
        "â€¢ åˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆRedis + Celeryï¼‰",
    ]
    
    for tip in tips:
        print(f"  {tip}")
    
    print()


async def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”"""
    print("=" * 60)
    print("ç¤ºä¾‹4: åŒæ­¥vså¼‚æ­¥æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)
    
    urls = [f"http://quotes.toscrape.com/page/{i}/" for i in range(1, 6)]
    
    print(f"\næµ‹è¯•: çˆ¬å– {len(urls)} ä¸ªé¡µé¢")
    
    # å¼‚æ­¥çˆ¬å–
    print("\nå¼‚æ­¥çˆ¬å–:")
    crawler = AsyncCrawler(max_concurrent=5)
    start_time = time.time()
    
    async with aiohttp.ClientSession() as session:
        crawler.session = session
        results = await crawler.fetch_all(urls)
    
    async_time = time.time() - start_time
    success_count = sum(1 for r in results if r)
    print(f"  è€—æ—¶: {async_time:.2f} ç§’")
    print(f"  æˆåŠŸ: {success_count}/{len(urls)} ä¸ªé¡µé¢")
    
    print("\næ€§èƒ½ä¼˜åŠ¿:")
    print(f"  â€¢ å¼‚æ­¥å¹¶å‘å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚")
    print(f"  â€¢ ä¸é˜»å¡ç­‰å¾…ï¼Œå……åˆ†åˆ©ç”¨IOç­‰å¾…æ—¶é—´")
    print(f"  â€¢ é€‚åˆIOå¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ï¼‰")
    
    print()


async def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("ğŸ“ æ¬¢è¿æ¥åˆ°ç¬¬å››å…³ï¼šä¸“ä¸šçº§çˆ¬è™«æ¡†æ¶")
    print("ğŸ“š åœ¨è¿™ä¸€å…³ï¼Œä½ å°†å­¦ä¹ æœ€ä¸“ä¸šçš„çˆ¬è™«æŠ€æœ¯")
    print("\n")
    
    try:
        await async_crawl_example()
        rate_limiter_example()
        architecture_example()
        await performance_comparison()
        
        print("=" * 60)
        print("ğŸ‰ æ­å–œï¼ä½ å·²å®Œæˆæ‰€æœ‰å…³å¡çš„å­¦ä¹ ")
        print("ğŸ’¡ ç°åœ¨ä½ å·²ç»æŒæ¡äº†ï¼š")
        print("   âœ“ å¼‚æ­¥å¹¶å‘çˆ¬å–")
        print("   âœ“ é€Ÿç‡é™åˆ¶")
        print("   âœ“ User-Agentè½®æ¢")
        print("   âœ“ ä¸“ä¸šçˆ¬è™«æ¶æ„è®¾è®¡")
        print("   âœ“ æ€§èƒ½ä¼˜åŒ–æŠ€å·§")
        print()
        print("ğŸ† ä½ å·²ç»ä»å°ç™½æˆé•¿ä¸ºä¸“ä¸šçš„ç½‘ç»œçˆ¬è™«å·¥ç¨‹å¸ˆï¼")
        print("ğŸš€ ç»§ç»­æ¢ç´¢æ›´å¤šé«˜çº§æŠ€æœ¯ï¼š")
        print("   â€¢ Scrapyæ¡†æ¶")
        print("   â€¢ SeleniumåŠ¨æ€ç½‘é¡µçˆ¬å–")
        print("   â€¢ åˆ†å¸ƒå¼çˆ¬è™«")
        print("   â€¢ éªŒè¯ç è¯†åˆ«")
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # Python 3.7+
    asyncio.run(main())
